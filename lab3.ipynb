{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN/5Z4zqxlHsLJCRNGhefzo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hqphuoc129/UIT-AI-Challenge2020/blob/master/lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDRO7mUQGDKC",
        "colab_type": "text"
      },
      "source": [
        "#Top1: BaiDu Team\n",
        "\n",
        "Method Used: Tracking by Detection\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyF-AmdvGPDU",
        "colab_type": "text"
      },
      "source": [
        "#Detection method: Paddle Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFwENJxB_Zao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/PaddlePaddle/PaddleDetection.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5K5lsjbY3VP",
        "colab_type": "text"
      },
      "source": [
        "Demo Paddle Detection\n",
        "\n",
        "IPYN Demo\n",
        "[link text](https://github.com/PaddlePaddle/PaddleDetection/blob/release/0.2/demo/mask_rcnn_demo.ipynb)\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/0.4/docs/images/000000570688.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5oRv9f_ZDs5",
        "colab_type": "text"
      },
      "source": [
        "#Tracking Method: modified DeepSort \n",
        "Prepare format for MOT:\n",
        "[link text](https://github.com/PaddlePaddle/Research/blob/master/CV/VehicleCounting/utils/prepare_for_mot.py)\n",
        "\n",
        "DeepSort Colab Tutorial:\n",
        "[link text](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/DeepSORT_YOLOv3.ipynb#scrollTo=s-swJb8AnUwf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36uJcvV-f0fY",
        "colab_type": "text"
      },
      "source": [
        "#Counting "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOfc5bExyvEy",
        "colab_type": "text"
      },
      "source": [
        "Code for calculating Distances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybx80MI2yy19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numba\n",
        "import numpy as np\n",
        "from math import sqrt, pow, cos, sin, asin\n",
        "\n",
        "@numba.jit(nopython=True, fastmath=True)\n",
        "def manhattan(array_x, array_y):\n",
        "\tn = array_x.shape[0]\n",
        "\tret = 0.\n",
        "\tfor i in range(n):\n",
        "\t\tret += abs(array_x[i]-array_y[i])\n",
        "\treturn ret\n",
        "\n",
        "@numba.jit(nopython=True, fastmath=True)\n",
        "def euclidean(array_x, array_y):\n",
        "\tn = array_x.shape[0]\n",
        "\tret = 0.\n",
        "\tfor i in range(n):\n",
        "\t\tret += (array_x[i]-array_y[i])**2\n",
        "\treturn sqrt(ret)\n",
        "\n",
        "@numba.jit(nopython=True, fastmath=True)\n",
        "def chebyshev(array_x, array_y):\n",
        "\tn = array_x.shape[0]\n",
        "\tret = -1*np.inf\n",
        "\tfor i in range(n):\n",
        "\t\td = abs(array_x[i]-array_y[i])\n",
        "\t\tif d>ret: ret=d\n",
        "\treturn ret\n",
        "\n",
        "@numba.jit(nopython=True, fastmath=True)\n",
        "def cosine(array_x, array_y):\n",
        "\tn = array_x.shape[0]\n",
        "\txy_dot = 0.\n",
        "\tx_norm = 0.\n",
        "\ty_norm = 0.\n",
        "\tfor i in range(n):\n",
        "\t\txy_dot += array_x[i]*array_y[i]\n",
        "\t\tx_norm += array_x[i]*array_x[i]\n",
        "\t\ty_norm += array_y[i]*array_y[i]\n",
        "\treturn 1.-xy_dot/(sqrt(x_norm)*sqrt(y_norm))\n",
        "\n",
        "@numba.jit(nopython=True, fastmath=True)\n",
        "def haversine(array_x, array_y):\n",
        "\tR = 6378.0\n",
        "\tradians = np.pi / 180.0\n",
        "\tlat_x = radians * array_x[0]\n",
        "\tlon_x = radians * array_x[1]\n",
        "\tlat_y = radians * array_y[0]\n",
        "\tlon_y = radians * array_y[1]\n",
        "\tdlon = lon_y - lon_x\n",
        "\tdlat = lat_y - lat_x\n",
        "\ta = (pow(sin(dlat/2.0), 2.0) + cos(lat_x) *\n",
        "\t\tcos(lat_y) * pow(sin(dlon/2.0), 2.0))\n",
        "\treturn R * 2 * asin(sqrt(a))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXR6vGoLyOn9",
        "colab_type": "text"
      },
      "source": [
        "Hausdoff Disance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q3mBPTnw4Rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import numba\n",
        "import distances\n",
        "from inspect import getmembers\n",
        "\n",
        "def _find_available_functions(module_name):\n",
        "    all_members = getmembers(module_name)\n",
        "    available_functions = [member[0] for member in all_members if isinstance(member[1], numba.targets.registry.CPUDispatcher)]\n",
        "    return available_functions\n",
        "\n",
        "@numba.jit(nopython=True, fastmath=True)\n",
        "def _hausdorff(XA, XB, distance_function):\n",
        "    nA = XA.shape[0]\n",
        "    nB = XB.shape[0]\n",
        "    cmax = 0.\n",
        "    for i in range(nA):\n",
        "        cmin = np.inf\n",
        "        for j in range(nB):\n",
        "            d = distance_function(XA[i,:], XB[j,:])\n",
        "            if d<cmin:\n",
        "                cmin = d\n",
        "            if cmin<cmax:\n",
        "                break\n",
        "        if cmin>cmax and np.inf>cmin:\n",
        "            cmax = cmin\n",
        "    '''\n",
        "\tfor j in range(nB):\n",
        "\t\tcmin = np.inf\n",
        "\t\tfor i in range(nA):\n",
        "\t\t\td = distance_function(XA[i,:], XB[j,:])\n",
        "\t\t\tif d<cmin:\n",
        "\t\t\t\tcmin = d\n",
        "\t\t\tif cmin<cmax:\n",
        "\t\t\t\tbreak\n",
        "\t\tif cmin>cmax and np.inf>cmin:\n",
        "\t\t\tcmax = cmin\n",
        "    '''\n",
        "    return cmax\n",
        "\n",
        "def hausdorff_distance(XA, XB, distance='euclidean'):\n",
        "    assert distance in _find_available_functions(distances), \\\n",
        "        'distance is not an implemented function'\n",
        "    assert type(XA) is np.ndarray and type(XB) is np.ndarray, \\\n",
        "        'arrays must be of type numpy.ndarray'\n",
        "    assert np.issubdtype(XA.dtype, np.number) and np.issubdtype(XA.dtype, np.number), \\\n",
        "        'the arrays data type must be numeric'\n",
        "    assert XA.ndim == 2 and XB.ndim == 2, \\\n",
        "        'arrays must be 2-dimensional'\n",
        "    assert XA.shape[1] == XB.shape[1], \\\n",
        "        'arrays must have equal number of columns'\n",
        "    if distance == 'haversine':\n",
        "        assert XA.shape[1] >= 2, 'haversine distance requires at least 2 coordinates per point (lat, lng)'\n",
        "        assert XB.shape[1] >= 2, 'haversine distance requires at least 2 coordinates per point (lat, lng)'\n",
        "    distance_function = getattr(distances, distance)\n",
        "    return _hausdorff(XA, XB, distance_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZDMI3lhyjTC",
        "colab_type": "text"
      },
      "source": [
        "Main code Counting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYTvBGFhycTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from hausdorff_dist import hausdorff_distance\n",
        "\n",
        "def check_bbox_inside_with_roi(bbox, mask):\n",
        "    #check if four point of bbox all in roi area\n",
        "    is_inside = True\n",
        "\n",
        "    x_tl = bbox[1]\n",
        "    y_tl = bbox[2]\n",
        "    x_br = bbox[3]\n",
        "    y_br = bbox[4]\n",
        "\n",
        "    for x in [x_tl, x_br]:\n",
        "        if x <= 0 or x >= mask.shape[1]:\n",
        "            return False\n",
        "\n",
        "    for y in [y_tl, y_br]:\n",
        "        if y <= 0 or y >= mask.shape[0]:\n",
        "            return False\n",
        "\n",
        "    vertexs = [[x_tl, y_tl], [x_tl, y_br], [x_br, y_tl], [x_br, y_br]]\n",
        "    for v in vertexs:\n",
        "        (g, b, r) = mask[v[1], v[0]]\n",
        "        if (g, b, r) < (128, 128, 128):\n",
        "            is_inside = False\n",
        "            return is_inside\n",
        "\n",
        "    return is_inside\n",
        "\n",
        "def check_tracks_with_roi(tracks, mask):\n",
        "    tracks_end_in_roi = []\n",
        "    tracks_start_in_roi = []\n",
        "    tracks_too_short = []\n",
        "\n",
        "    for trackid, track in tracks.items():\n",
        "        start_bbox = track['bbox'][0]\n",
        "        end_bbox = track['bbox'][-1]\n",
        "\n",
        "        if check_bbox_inside_with_roi(start_bbox, mask) == True:\n",
        "            if track['startframe'] > 3:\n",
        "                tracks_start_in_roi.append(trackid)\n",
        "\n",
        "        if check_bbox_inside_with_roi(end_bbox, mask) == True:\n",
        "            tracks_end_in_roi.append(trackid)\n",
        "\n",
        "        if track['endframe'] - track['startframe'] < 10:\n",
        "            if trackid not in tracks_start_in_roi:\n",
        "                tracks_too_short.append(trackid)\n",
        "    return tracks_end_in_roi, tracks_start_in_roi, tracks_too_short\n",
        "\n",
        "\n",
        "def check_bbox_overlap_with_roi(bbox, mask):\n",
        "    is_overlap = False\n",
        "    if bbox[1] >= mask.shape[1] or bbox[2] >= mask.shape[0] \\\n",
        "            or bbox[3] < 0 or bbox[4] < 0:\n",
        "        return is_overlap\n",
        "\n",
        "    x_tl = bbox[1] if bbox[1] > 0 else 0\n",
        "    y_tl = bbox[2] if bbox[2] > 0 else 0\n",
        "    x_br = bbox[3] if bbox[3] < mask.shape[1] else mask.shape[1] - 1\n",
        "    y_br = bbox[4] if bbox[4] < mask.shape[0] else mask.shape[0] - 1\n",
        "    vertexs = [[x_tl, y_tl], [x_tl, y_br], [x_br, y_tl], [x_br, y_br]]\n",
        "    for v in vertexs:\n",
        "        (g, b, r) = mask[v[1], v[0]]\n",
        "        if (g, b, r) > (128, 128, 128):\n",
        "            is_overlap = True\n",
        "            return is_overlap\n",
        "\n",
        "    return is_overlap\n",
        "\n",
        "def is_same_direction(traj1, traj2, angle_thr):\n",
        "    vec1 = np.array([traj1[-1][0] - traj1[0][0], traj1[-1][1] - traj1[0][1]])\n",
        "    vec2 = np.array([traj2[-1][0] - traj2[0][0], traj2[-1][1] - traj2[0][1]])\n",
        "    L1 = np.sqrt(vec1.dot(vec1))\n",
        "    L2 = np.sqrt(vec2.dot(vec2))\n",
        "    if L1 == 0 or L2 == 0:\n",
        "        return False\n",
        "    cos = vec1.dot(vec2)/(L1*L2)\n",
        "    angle = np.arccos(cos) * 360/(2*np.pi)\n",
        "    if angle < angle_thr:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def calc_angle(vec1, vec2):\n",
        "    vec1 = np.array([traj1[-1][0] - traj1[-5][0], traj1[-1][1] - traj1[-5][1]])\n",
        "    vec2 = np.array([traj2[-1][0] - traj2[-5][0], traj2[-1][1] - traj2[-5][1]])\n",
        "    L1 = np.sqrt(vec1.dot(vec1))\n",
        "    L2 = np.sqrt(vec2.dot(vec2))\n",
        "    if L1 == 0 or L2 == 0:\n",
        "        return 90\n",
        "    cos = vec1.dot(vec2)/(L1*L2)\n",
        "    if cos > 1:\n",
        "        return 90\n",
        "    angle = np.arccos(cos) * 360/(2*np.pi)\n",
        "    return angle\n",
        "\n",
        "def count_video(data_root, video_name, save_root):\n",
        "    '''\n",
        "    data_root: the path which contains (1)the track_results(2)masks(3)AIC20_track1(4)cam-configs\n",
        "    video_name: the video to count\n",
        "    save_root: the root path to save counting results\n",
        "    '''\n",
        "    #get cam_name related index\n",
        "    #for datasetB, you shold change the path for video_list_id.txt \n",
        "    video_list_file = os.path.join(data_root, 'AIC20_track1/Dataset_A/list_video_id.txt')\n",
        "    if not os.path.exists(video_list_file):\n",
        "        print('no list_video_id.txt found! ')\n",
        "        return\n",
        "        \n",
        "    with open(video_list_file, 'r') as fl:\n",
        "        lines = [line.strip('\\n').split(' ') for line in fl]\n",
        "        for line in lines:\n",
        "            if line[1][:-4] != video_name:\n",
        "                continue\n",
        "            else:\n",
        "                video_idx = line[0]\n",
        "\n",
        "    # load movements tipical trajs\n",
        "    cam_name = 'cam_' + video_name.split('_')[1]\n",
        "    cam_conf = os.path.join(data_root, 'cam-configs', cam_name+'.json')\n",
        "    tipical_trajs = {}\n",
        "    with open(cam_conf, 'r') as fc:\n",
        "        movements = json.load(fc)\n",
        "        for movement_id, movement_info in movements.items():\n",
        "            try:\n",
        "                tracklets = movement_info['tracklets']\n",
        "                tipical_trajs[movement_id] = tracklets\n",
        "            except:\n",
        "                print('cam config error!')\n",
        "                return\n",
        "\n",
        "    #read mask image\n",
        "    cam_mask = os.path.join(data_root, 'mask', video_name+'.jpg')\n",
        "    mask = cv2.imread(cam_mask)\n",
        "    h, w, c = mask.shape\n",
        "    #load tracks\n",
        "    tracks = {}\n",
        "    #path to track results path\n",
        "    track_file = os.path.join(data_root, 'track_results', video_name+'.txt')\n",
        "    with open(track_file, 'r') as ft:\n",
        "        lines = [line.strip('\\n').split(' ') for line in ft]\n",
        "        for line in lines:\n",
        "            frameid = int(line[0])\n",
        "            trackid = int(line[1])\n",
        "            x1 = int(float(line[2]))\n",
        "            y1 = int(float(line[3]))\n",
        "            x2 = int(float(line[4]))\n",
        "            y2 = int(float(line[5]))\n",
        "            cx = int((x1 + x2) / 2)\n",
        "            cy = int((y1 + y2) / 2)\n",
        "\n",
        "            label = line[6]\n",
        "            if trackid in tracks:\n",
        "                tracks[trackid]['endframe'] = frameid\n",
        "                tracks[trackid]['bbox'].append([frameid, x1, y1, x2, y2, label])\n",
        "                tracks[trackid]['tracklet'].append([cx, cy])\n",
        "            else:\n",
        "                tracks[trackid] = {'startframe' : frameid,\n",
        "                                   'endframe' : frameid,\n",
        "                                   'bbox' : [[frameid, x1, y1, x2, y2, label]],\n",
        "                                   'tracklet' : [[cx, cy]]}\n",
        "\n",
        "    #split tracklets\n",
        "    tracks_end_in_roi, tracks_start_in_roi, tracks_too_short = check_tracks_with_roi(tracks, mask)\n",
        "\n",
        "    trackids = sorted([k for k in tracks.keys()])\n",
        "    #save count results\n",
        "    os.makedirs(save_root, exist_ok=True)\n",
        "    savefile = os.path.join(save_root, video_name+'.txt')\n",
        "    dst_out = open(savefile, 'w')\n",
        "\n",
        "    #start counting\n",
        "    dist_thr = 300\n",
        "    angle_thr = 30\n",
        "    min_length = 10\n",
        "    results = []\n",
        "    for trackid in trackids:\n",
        "        if len(tracks[trackid]['tracklet']) < min_length:\n",
        "            continue\n",
        "        track_traj = tracks[trackid]['tracklet']\n",
        "        #calc hausdorff dist with tipical trajs, assign the movement with the min dist\n",
        "        all_dists_dict = {k: float('inf') for k in tipical_trajs}\n",
        "        for m_id, m_t in tipical_trajs.items():\n",
        "            for t in m_t:\n",
        "                tmp_dist = hausdorff_distance(np.array(track_traj), np.array(t), distance='euclidean')\n",
        "                if tmp_dist < all_dists_dict[m_id]:\n",
        "                    all_dists_dict[m_id] = tmp_dist\n",
        "\n",
        "        #check direction\n",
        "        all_dists = sorted(all_dists_dict.items(), key=lambda k: k[1])\n",
        "        min_idx, min_dist = None, dist_thr\n",
        "        for i in range(0, len(all_dists)):\n",
        "            m_id = all_dists[i][0]\n",
        "            m_dist = all_dists[i][1]\n",
        "            if m_dist >= dist_thr: #if min dist > dist_thr, will not assign to any movement\n",
        "                break\n",
        "            else:\n",
        "                if is_same_direction(track_traj, tipical_trajs[m_id][0], angle_thr): #check direction\n",
        "                    min_idx = m_id\n",
        "                    min_dist = m_dist\n",
        "                    break #if match, end\n",
        "                else:\n",
        "                    continue #direction not matched, find next m_id\n",
        "\n",
        "        #cam_13 14 will not use shape based method\n",
        "        direct_match_videos = ['cam_13', 'cam_14']\n",
        "        if cam_name not in direct_match_videos and min_idx == None and min_dist >= dist_thr:\n",
        "            continue\n",
        "\n",
        "        #spatial constrains\n",
        "        #-----------------------------------------------------------------------------------\n",
        "        #cam 1\n",
        "        if cam_name == 'cam_1' and min_idx in ['movement_1', 'movement_2', 'movement_3'] and \\\n",
        "                len(track_traj) < 30:\n",
        "            continue\n",
        "        #is mv3\n",
        "        if cam_name == 'cam_1' and min_idx != 'movement_3':\n",
        "            cx, cy = track_traj[0]\n",
        "            if cx > w * 0.5 and cy > h * 0.3:\n",
        "                min_idx = 'movement_3'\n",
        "        #is mv1 or 2 not 3\n",
        "        if cam_name == 'cam_1' and min_idx == 'movement_3':\n",
        "            cx, cy = track_traj[0]\n",
        "            if cx < w * 0.5 or cy < h * 0.3:\n",
        "                if is_same_direction(track_traj, tipical_trajs['movement_1'][0], 45):\n",
        "                    min_idx = 'movement_1'\n",
        "                elif is_same_direction(track_traj, tipical_trajs['movement_2'][0], 45):\n",
        "                    min_idx = 'movement_2'\n",
        "\n",
        "        #-----------------------------------------------------------------------------------\n",
        "        #cam 2\n",
        "        if cam_name == 'cam_2' and min_idx in ['movement_1', 'movement_2'] and \\\n",
        "                len(track_traj) < 30:\n",
        "                    continue\n",
        "        #-----------------------------------------------------------------------------------\n",
        "        #cam_3\n",
        "        #is mv4\n",
        "        if cam_name == 'cam_3' and min_idx != 'movement_4':\n",
        "            cx, cy = track_traj[0]\n",
        "            if cx > w * 0.5 and cy > h * 0.5:\n",
        "                min_idx = 'movement_4'\n",
        "        #is mv 1 2 3 not mv4\n",
        "        if cam_name == 'cam_3' and min_idx == 'movement_4':\n",
        "            cx, cy = track_traj[0]\n",
        "            if cx < w * 0.5 or cy < h * 0.5:\n",
        "                if is_same_direction(track_traj, tipical_trajs['movement_1'][0], 45):\n",
        "                    min_idx = 'movement_1'\n",
        "                elif is_same_direction(track_traj, tipical_trajs['movement_2'][0], 45):\n",
        "                    min_idx = 'movement_2'\n",
        "                elif is_same_direction(track_traj, tipical_trajs['movement_3'][0], 45):\n",
        "                    min_idx = 'movement_3'\n",
        "        #-----------------------------------------------------------------------------------\n",
        "        #cam4 5\n",
        "        # is mv5 not mv 1 12\n",
        "        if cam_name in [\"cam_4\", 'cam_5'] and min_idx in [\"movement_1\", 'movement_12']:\n",
        "            cx, cy = track_traj[0]\n",
        "            if cx > 300:\n",
        "                if all_dists_dict['movement_5'] < dist_thr:\n",
        "                    min_idx = 'movement_5'\n",
        "                else:\n",
        "                    continue\n",
        "        #is mv8 not mv4\n",
        "        if cam_name in ['cam_4', 'cam_5'] and min_idx == 'movement_4':\n",
        "            cx, cy = track_traj[0]\n",
        "            if cx > 1000:\n",
        "                if all_dist_dict['movement_8'] < dist_thr:\n",
        "                    min_idx = 'movement_8'\n",
        "                else:\n",
        "                    continue\n",
        "        #is mv5 not mv4\n",
        "        if cam_name in ['cam_4', 'cam_5'] and min_idx == 'movement_4':\n",
        "            #print(trackid)\n",
        "            cx, cy = track_traj[-1]\n",
        "            if cx < 100:\n",
        "                if all_dist_dict['movement_5'] < dist_thr:\n",
        "                    min_idx = 'movement_5'\n",
        "                else:\n",
        "                    continue\n",
        "        if cam_name in ['cam_4', 'cam_5'] and min_idx in ['movement_3'] and len(track_traj) < 30:\n",
        "            continue\n",
        "        if cam_name in ['cam_4', 'cam_5'] and min_idx in ['movement_5'] and \\\n",
        "                trackid in tracks_start_in_roi:\n",
        "            continue\n",
        "\n",
        "        #------------------------------------------------------------------------------------\n",
        "        if cam_name == 'cam_7' and trackid in tracks_end_in_roi:\n",
        "            continue\n",
        "\n",
        "        #------------------------------------------------------------------------------------\n",
        "        #cam 10 11\n",
        "        if cam_name in ['cam_10', 'cam_11'] and min_idx == 'movement_1':\n",
        "            if track_traj[0][0] > w * 0.5:\n",
        "                continue\n",
        "        if cam_name in ['cam_10', 'cam_11'] and min_idx == 'movement_3':\n",
        "            if track_traj[-1][0] < 200:\n",
        "                continue\n",
        "        #------------------------------------------------------------------------------------\n",
        "        #cam12\n",
        "        if cam_name == 'cam_12' and min_idx in ['movement_1', 'movement_2']:\n",
        "            if track_traj[0][0] > w * 0.6 or track_traj[0][0] < 100 or track_traj[0][1] > h * 0.5:\n",
        "                continue\n",
        "            if len(track_traj) < 30:\n",
        "                continue\n",
        "            #if min_idx == 'movement_2' and track_traj[-1][0] and track_traj[-1][1] < 600:\n",
        "            #    print(trackid, track_traj[-1])\n",
        "        #------------------------------------------------------------------------------------\n",
        "        #cam13\n",
        "        if cam_name == 'cam_13':\n",
        "            if track_traj[0][1] > 374 or track_traj[0][0] > w * 0.7 or track_traj[0][1] > h * 0.8:\n",
        "                continue\n",
        "            if track_traj[-1][0] < w * 0.7 and track_traj[-1][1] > h * 0.8:\n",
        "                min_idx = 'movement_2'\n",
        "            elif track_traj[-1][0] > w * 0.7 and track_traj[-1][1] > h * 0.5:\n",
        "                min_idx = 'movement_3'\n",
        "            elif track_traj[-1][0] < w * 0.3 and track_traj[-1][1] < h * 0.8:\n",
        "                min_idx = 'movement_1'\n",
        "            else:\n",
        "                continue\n",
        "        #------------------------------------------------------------------------------------\n",
        "        #cam14\n",
        "        if cam_name == 'cam_14':\n",
        "            if track_traj[-1][0] <= w * 0.5 and is_same_direction(track_traj,\n",
        "                    tipical_trajs['movement_1'][0], 45):\n",
        "                min_idx = 'movement_1'\n",
        "            elif track_traj[-1][0] > w * 0.5 and is_same_direction(track_traj, tipical_trajs['movement_2'][0], 45):\n",
        "                min_idx = 'movement_2'\n",
        "            elif abs(track_traj[-1][0] - track_traj[0][0]) < 100:\n",
        "            # for id switch in remote area which lead to the trackid go back \n",
        "                pass\n",
        "            else:\n",
        "                continue\n",
        "        #------------------------------------------------------------------------------------\n",
        "        if cam_name == 'cam_15':\n",
        "            #remove still vehicle\n",
        "            track_w = tracks[trackid]['bbox'][0][3] - tracks[trackid]['bbox'][0][1]\n",
        "            if abs(track_traj[-1][0] - track_traj[0][0]) < 2*track_w:\n",
        "                continue\n",
        "            if track_traj[0][0] < w * 0.5:\n",
        "                min_idx = 'movement_1'\n",
        "            elif track_traj[0][0] > w * 0.7:\n",
        "                min_idx = 'movement_2'\n",
        "            else:\n",
        "                continue\n",
        "        #------------------------------------------------------------------------------------\n",
        "        #cam_20\n",
        "        if cam_name in ['cam_19', 'cam_20']:\n",
        "            #remove still vehicle\n",
        "            track_h = tracks[trackid]['bbox'][0][4] - tracks[trackid]['bbox'][0][2]\n",
        "            if abs(track_traj[-1][1] - track_traj[0][1]) < track_h:\n",
        "                continue\n",
        "\n",
        "        #save counting results\n",
        "        mv_idx = min_idx.split('_')[1]\n",
        "        #get last frameid in roi\n",
        "        bboxes = tracks[trackid]['bbox']\n",
        "        bboxes.sort(key=lambda x: x[0])\n",
        "\n",
        "        dst_frame = bboxes[0][0]\n",
        "        last_bbox = bboxes[-1]\n",
        "        if check_bbox_overlap_with_roi(last_bbox, mask) == True:\n",
        "            dst_frame = last_bbox[0]\n",
        "        else:\n",
        "            for i in range(len(bboxes) - 2, 0, -1):\n",
        "                bbox = bboxes[i]\n",
        "                if check_bbox_overlap_with_roi(bbox, mask) == True:\n",
        "                    dst_frame = bbox[0]\n",
        "                    break\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "        track_types = [k[5] for k in bboxes]\n",
        "        track_type = max(track_types, key=track_types.count)\n",
        "\n",
        "        if track_type == 'car':\n",
        "            cls_id = 1\n",
        "        else:\n",
        "            cls_id = 2\n",
        "        results.append([video_idx, dst_frame, mv_idx, cls_id])\n",
        "    #save\n",
        "    results.sort(key=lambda x: (x[1], x[2]))\n",
        "    for res in results:\n",
        "        res_str = \" \".join(str(k) for k in res)\n",
        "        print(res_str, file=dst_out)\n",
        "    dst_out.close()\n",
        "    print('vehicle counting done.')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data_root = '../'\n",
        "    save_root = './vehicle_counting_results'\n",
        "    #video_list = os.listdir(os.path.join(data_root, 'imageset'))\n",
        "    video_list = [sys.argv[1]]\n",
        "\n",
        "    for video_name in video_list:\n",
        "        print('start to counting video %s ... ' % video_name)\n",
        "        count_video(data_root, video_name, save_root)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW4KVC98f5p9",
        "colab_type": "text"
      },
      "source": [
        "#Top 2: \n",
        "\n",
        "Method Used: Tracktor + Movement Classification + Vehicle Type Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llf9hlxPhAIm",
        "colab_type": "text"
      },
      "source": [
        "#Tracking\n",
        "Tracking without bell and whistles\n",
        "[link text](https://github.com/phil-bergmann/tracking_wo_bnw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMyggsEwz3RK",
        "colab_type": "text"
      },
      "source": [
        "Google Colab Tutorials:\n",
        "\n",
        "[link text](https://colab.research.google.com/drive/1oe_vdQQUMSG3XHp1ki7kiE3HA2fSXkO-)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQSe7KkS3pwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from intersection_class import counter_classification\n",
        "\n",
        "class Countor:\n",
        "    \"\"\"The main tracking file, here is where magic happens.\"\"\"\n",
        "\n",
        "    def __init__(self, obj_detect, calibration, device):\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        # area minimal\n",
        "        obj_detect.det_min_area = calibration.calibration['detection_min_area_val']\n",
        "        obj_detect.tck_min_area = calibration.calibration['tracks_min_area_val']\n",
        "        \n",
        "        # area maximal\n",
        "        obj_detect.det_max_area = calibration.calibration['detection_max_area_val']\n",
        "        \n",
        "        # thresh score\n",
        "        #print(self.roi_heads.score_thresh)\n",
        "        obj_detect.det_score_thresh = calibration.calibration['detection_object_thresh']\n",
        "        obj_detect.tck_score_thresh = calibration.calibration['tracks_object_thresh']\n",
        "        \n",
        "        # nms thresh\n",
        "        #print(self.roi_heads.nms_thresh)\n",
        "        obj_detect.det_nms_thresh = calibration.calibration['detection_nms_thresh']\n",
        "        obj_detect.tck_nms_thresh = calibration.calibration['tracks_nms_thresh']\n",
        "        #self.empty_var = torch.empty(0, device=device)\n",
        "        \n",
        "        # porcentage ROI in\n",
        "        obj_detect.det_min_ROI_in = calibration.calibration['detection_ROI_in']\n",
        "        obj_detect.tck_min_ROI_in = calibration.calibration['tracks_ROI_in']\n",
        "                \n",
        "        # networks\n",
        "        self.obj_detect = obj_detect\n",
        "        \n",
        "        # ROI image\n",
        "        self.ROI_tensor = calibration.ROI_tensor.to(device)\n",
        "        \n",
        "        # Movement classification\n",
        "        self.Movement_classification = counter_classification(calibration.movements)\n",
        "        \n",
        "        # Countor\n",
        "        self.min_length_trajectory = calibration.calibration['min_length_trajectory']\n",
        "        \n",
        "        # motion model parameters\n",
        "        self.motion_model_cfg  = calibration.calibration['motion_model']\n",
        "      \n",
        "        # set the values to \n",
        "        self.reset()\n",
        "\n",
        "    def reset(self, hard=True):\n",
        "        self.tracks = []\n",
        "        self.inactive_tracks = []\n",
        "\n",
        "        if hard:\n",
        "            self.track_num = 0\n",
        "            self.results = {}\n",
        "            self.countor_restults = {}\n",
        "            self.im_index = 0\n",
        "\n",
        "    def tracks_to_inactive(self, tracks):\n",
        "        self.tracks = [t for t in self.tracks if t not in tracks]\n",
        "        for t in tracks:\n",
        "            t.pos = t.last_pos[-1]          \n",
        "            vector   = t.get_vector().cpu().numpy()\n",
        "            movement = self.Movement_classification.classify(vector[:-1])\n",
        "            self.countor_restults[t.id.cpu().numpy().item()] =np.concatenate([np.array([self.im_index]).astype(int),movement,vector])\n",
        "        self.inactive_tracks += tracks\n",
        "\n",
        "    def add(self, new_det_pos, new_det_scores):\n",
        "        \"\"\"Initializes new Track objects and saves them.\"\"\"\n",
        "        num_new = new_det_pos.size(0)\n",
        "        for i in range(num_new):\n",
        "            self.tracks.append(Track(\n",
        "                new_det_pos[i].view(1, -1),\n",
        "                new_det_scores[i],\n",
        "                torch.tensor([self.track_num + i]).cuda(),\n",
        "                self.motion_model_cfg['n_steps'] if self.motion_model_cfg['n_steps'] > 0 else 1\n",
        "            ))\n",
        "        self.track_num += num_new\n",
        "    \n",
        "    def get_pos(self):\n",
        "        \"\"\"Get the positions of all active tracks.\"\"\"\n",
        "        if len(self.tracks) == 1:\n",
        "            pos = self.tracks[0].pos\n",
        "            ids = self.tracks[0].id\n",
        "        elif len(self.tracks) > 1:\n",
        "            pos = torch.cat([t.pos for t in self.tracks], 0)\n",
        "            ids = torch.cat([t.id for t in self.tracks], 0)\n",
        "        else:\n",
        "            pos = torch.empty(0, device=self.device)\n",
        "            ids = torch.empty(0, device=self.device)\n",
        "        return pos,ids\n",
        "\n",
        "    def motion_step(self, track):\n",
        "        \"\"\"Updates the given track's position by one step based on track.last_v\"\"\"\n",
        "        if self.motion_model_cfg['center_only']:\n",
        "            center_new = get_center(track.pos) + track.last_v\n",
        "            track.pos = make_pos(*center_new, get_width(track.pos), get_height(track.pos)).to(self.device)\n",
        "        else:\n",
        "            track.pos = track.pos + track.last_v\n",
        "\n",
        "    def motion(self):\n",
        "        \"\"\"Applies a simple linear motion model that considers the last n_steps steps.\"\"\"\n",
        "        for t in self.tracks:\n",
        "            last_pos = list(t.last_pos)\n",
        "\n",
        "            # avg velocity between each pair of consecutive positions in t.last_pos\n",
        "            if self.motion_model_cfg['center_only']:\n",
        "                vs = [get_center(p2).to(self.device) - get_center(p1).to(self.device) for p1, p2 in zip(last_pos, last_pos[1:])]\n",
        "            else:\n",
        "                vs = [p2 - p1 for p1, p2 in zip(last_pos, last_pos[1:])]\n",
        "\n",
        "            t.last_v = torch.stack(vs).mean(dim=0)\n",
        "            self.motion_step(t)\n",
        "\n",
        "                    \n",
        "    def step(self, image):\n",
        "        \"\"\"This function should be called every timestep to perform tracking with a blob\n",
        "        containing the image information.\n",
        "        \"\"\"\n",
        "        for t in self.tracks:\n",
        "            # add current position to last_pos list\n",
        "            t.last_pos.append(t.pos.clone())\n",
        "\n",
        "        # apply motion model\n",
        "        if len(self.tracks):\n",
        "            # apply motion model\n",
        "            if self.motion_model_cfg['enabled']:\n",
        "                self.motion()\n",
        "                self.tracks = [t for t in self.tracks if t.has_positive_area()]\n",
        "                \n",
        "        pos, ids =self.get_pos()\n",
        "        \n",
        "        # Run the neural network\n",
        "        det_boxes, det_scores, det_labels, tck_boxes, tck_scores, tck_labels, tck_ids = self.obj_detect(image, \n",
        "                                                                                                        [pos],\n",
        "                                                                                                        [ids],\n",
        "                                                                                                 [self.ROI_tensor])\n",
        "\n",
        "        det_boxes = det_boxes[0].detach()\n",
        "        det_scores = det_scores[0].detach()\n",
        "        det_labels = det_labels[0].detach()\n",
        "        tck_boxes = tck_boxes[0].detach()\n",
        "        tck_scores = tck_scores[0].detach()\n",
        "        tck_labels = tck_labels[0].detach()\n",
        "        tck_ids = tck_ids[0].detach()\n",
        "        \n",
        "        # update position and score for the current tracks\n",
        "        tracks_to_inactive_list = []\n",
        "        if len(self.tracks):\n",
        "            for i in range(len(self.tracks) - 1, -1, -1):\n",
        "                if self.tracks[i].id in tck_ids:\n",
        "                    index = tck_ids == self.tracks[i].id\n",
        "                    index = index.nonzero().squeeze(1)\n",
        "                    \n",
        "                    self.tracks[i].pos   = tck_boxes[index].view(1, -1)\n",
        "                    self.tracks[i].score = tck_scores[index]\n",
        "                else:\n",
        "                    # if the index is not in the ids set to inactive\n",
        "                    tracks_to_inactive_list.append(self.tracks[i])\n",
        "\n",
        "        # set the intective tracks  \n",
        "        self.tracks_to_inactive(tracks_to_inactive_list)\n",
        "        \n",
        "        # Create new tracks\n",
        "        if det_boxes.nelement() > 0:\n",
        "            self.add(det_boxes, det_scores)\n",
        "                \n",
        "\n",
        "        ####################\n",
        "        # Generate Results #\n",
        "        ####################\n",
        "        for t in self.tracks:\n",
        "            if t.id.cpu().numpy().item() not in self.results.keys():\n",
        "                self.results[t.id.cpu().numpy().item()] = {}\n",
        "            self.results[t.id.cpu().numpy().item()][self.im_index] = np.concatenate([t.pos[0].cpu().numpy(), np.array([t.score.cpu()])])\n",
        "        \n",
        "        ####################\n",
        "        # Prepare next frame #\n",
        "        ####################\n",
        "        self.im_index += 1\n",
        "        self.last_image = image[0]   \n",
        "        \n",
        "    def get_results(self):\n",
        "        return self.results\n",
        "\n",
        "\n",
        "class Track(object):\n",
        "    \"\"\"This class contains all necessary for every individual track.\"\"\"\n",
        "\n",
        "    def __init__(self, pos, score, track_id, mm_steps):\n",
        "        self.id = track_id\n",
        "        self.pos = pos\n",
        "        self.init_pos = pos\n",
        "        self.score = score\n",
        "        self.ims = deque([])\n",
        "        self.last_pos = deque([pos.clone()], maxlen=mm_steps + 1)\n",
        "        self.last_v = torch.Tensor([])\n",
        "\n",
        "\n",
        "    def has_positive_area(self):\n",
        "        return self.pos[0, 2] > self.pos[0, 0] and self.pos[0, 3] > self.pos[0, 1]\n",
        "        \n",
        "    def get_vector(self):\n",
        "        x1y1 = get_center(self.init_pos)\n",
        "        x2y2 = get_center(self.pos)\n",
        "        dist = torch.dist(x1y1, x2y2,2).unsqueeze(0)\n",
        "        return torch.cat([x1y1,x2y2,dist])\n",
        "\n",
        "    \n",
        "def get_center(pos):\n",
        "    x1 = pos[0, 0]\n",
        "    y1 = pos[0, 1]\n",
        "    x2 = pos[0, 2]\n",
        "    y2 = pos[0, 3]\n",
        "    return torch.Tensor([(x2 + x1) / 2, (y2 + y1) / 2]).cuda()\n",
        "\n",
        "\n",
        "def get_width(pos):\n",
        "    return pos[0, 2] - pos[0, 0]\n",
        "\n",
        "\n",
        "def get_height(pos):\n",
        "    return pos[0, 3] - pos[0, 1]\n",
        "\n",
        "\n",
        "def make_pos(cx, cy, width, height):\n",
        "    return torch.Tensor([[\n",
        "        cx - width / 2,\n",
        "        cy - height / 2,\n",
        "        cx + width / 2,\n",
        "        cy + height / 2\n",
        "    ]]).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSdgSivh2pt4",
        "colab_type": "text"
      },
      "source": [
        "#Countor Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTXstR-K2twx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "from torchvision.models.detection.transform import resize_boxes\n",
        "from torchvision.ops.boxes import clip_boxes_to_image, nms\n",
        "\n",
        "class Countor_NN(FasterRCNN):\n",
        "\n",
        "    def __init__(self):\n",
        "        backbone = resnet_fpn_backbone('resnet50', False)\n",
        "        super(Countor_NN, self).__init__(backbone, 91)\n",
        "        # get cars trucks and bus classes\n",
        "        self.selected_classes = [3,6,8]\n",
        "        \n",
        "        # area minimal\n",
        "        self.det_min_area = 100\n",
        "        self.tck_min_area = 100\n",
        "        \n",
        "        # area maximal\n",
        "        self.det_max_area = 1000000\n",
        "        \n",
        "        # thresh score\n",
        "        #print(self.roi_heads.score_thresh)\n",
        "        self.det_score_thresh = 0.5\n",
        "        self.tck_score_thresh = 0.05\n",
        "        \n",
        "        # nms thresh\n",
        "        #print(self.roi_heads.nms_thresh)\n",
        "        self.det_nms_thresh = 0.5\n",
        "        self.tck_nms_thresh = 0.5\n",
        "        #self.empty_var = torch.empty(0, device=device)\n",
        "        \n",
        "        # porcentage ROI in\n",
        "        self.det_min_ROI_in = 0.3\n",
        "        self.tck_min_ROI_in = 0.1\n",
        "        \n",
        "    def get_parameters(self):\n",
        "        \n",
        "        parameters = {}\n",
        "        # area minimal\n",
        "        parameters['det_min_area'] = self.det_min_area \n",
        "        parameters['tck_min_area'] = self.tck_min_area \n",
        "        \n",
        "        # area maximal\n",
        "        parameters['det_max_area'] = self.det_max_area \n",
        "        \n",
        "        # thresh score\n",
        "        #print(self.roi_heads.score_thresh)\n",
        "        parameters['det_score_thresh'] = self.det_score_thresh \n",
        "        parameters['tck_score_thresh'] = self.tck_score_thresh \n",
        "        \n",
        "        # nms thresh\n",
        "        #print(self.roi_heads.nms_thresh)\n",
        "        parameters['det_nms_thresh'] = self.det_nms_thresh \n",
        "        parameters['tck_nms_thresh'] = self.tck_nms_thresh \n",
        "        #self.empty_var = torch.empty(0, device=device)\n",
        "        \n",
        "        # porcentage ROI in\n",
        "        parameters['det_min_ROI_in'] = self.det_min_ROI_in\n",
        "        parameters['tck_min_ROI_in'] = self.tck_min_ROI_in\n",
        "        \n",
        "        return parameters\n",
        "        \n",
        "    @staticmethod\n",
        "    def remove_small_boxes_area(boxes, min_size):\n",
        "        \n",
        "        # type: (Tensor, float)\n",
        "        \"\"\"\n",
        "        Remove boxes which area is smaller than min_size.\n",
        "        Arguments:\n",
        "            boxes (Tensor[N, 4]): boxes in (x1, y1, x2, y2) format\n",
        "            min_size (float): minimum size\n",
        "        Returns:\n",
        "            keep (Tensor[K]): indices of the boxes that have both sides\n",
        "                larger than min_size\n",
        "        \"\"\"\n",
        "        area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
        "        keep = area >= min_size\n",
        "        keep = keep.nonzero().squeeze(1)\n",
        "        return keep\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_big_boxes_area(boxes, max_size):\n",
        "        \n",
        "        # type: (Tensor, float)\n",
        "        \"\"\"\n",
        "        Remove boxes which area is smaller than min_size.\n",
        "        Arguments:\n",
        "            boxes (Tensor[N, 4]): boxes in (x1, y1, x2, y2) format\n",
        "            min_size (float): minimum size\n",
        "        Returns:\n",
        "            keep (Tensor[K]): indices of the boxes that have both sides\n",
        "                larger than min_size\n",
        "        \"\"\"\n",
        "        area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
        "        keep = area <= max_size\n",
        "        keep = keep.nonzero().squeeze(1)\n",
        "        return keep\n",
        "            \n",
        "    @staticmethod\n",
        "    def remove_boxes_out_roi(boxes, ROI_image, min_in_porcentage):\n",
        "        \n",
        "        # type: (Tensor, float)\n",
        "        \"\"\"\n",
        "        Remove boxes which contains at least one side smaller than min_size.\n",
        "        Arguments:\n",
        "            boxes (Tensor[N, 4]): boxes in (x1, y1, x2, y2) format\n",
        "            min_size (float): minimum size\n",
        "        Returns:\n",
        "            keep (Tensor[K]): indices of the boxes that have both sides\n",
        "                larger than min_size\n",
        "        \"\"\"\n",
        "        \n",
        "        area_in =[ROI_image[box[1]:box[3],box[0]:box[2]].sum() for box in boxes.int()]\n",
        "        area_in = torch.stack(area_in) \n",
        "        area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
        "        porcentage_in = area_in/area.float()\n",
        "        keep = porcentage_in >= min_in_porcentage\n",
        "        keep = keep.nonzero().squeeze(1)\n",
        "        return keep\n",
        "    \n",
        "    # detections roi head\n",
        "\n",
        "    def detections_processing(self, images, features, ROI_images, tck_boxes, original_image_sizes):\n",
        "        # rpn network\n",
        "        det_proposals, proposal_losses = self.rpn(images, features)\n",
        "\n",
        "        # roi heads to get boxes and scores\n",
        "        det_box_features = self.roi_heads.box_roi_pool(features, det_proposals, images.image_sizes)\n",
        "        det_box_features = self.roi_heads.box_head(det_box_features)\n",
        "        det_class_logits, det_box_regression = self.roi_heads.box_predictor(det_box_features)\n",
        "        det_boxes = self.roi_heads.box_coder.decode(det_box_regression, det_proposals)\n",
        "        det_scores = F.softmax(det_class_logits, -1)\n",
        "\n",
        "\n",
        "        boxes_per_image = [boxes_in_image.shape[0] for boxes_in_image in det_proposals]\n",
        "\n",
        "        # this gets the max score and gives the label\n",
        "        # this mean just for 3 classes get just one\n",
        "        det_scores, det_labels = det_scores[:,self.selected_classes].max(1)\n",
        "        det_boxes  = det_boxes[:,self.selected_classes]\n",
        "        det_boxes = torch.cat([det_boxes[idx][i].unsqueeze(0)for idx, i in enumerate(det_labels)])\n",
        "\n",
        "        # split the boxes scores and labels for each image for post processing\n",
        "        det_boxes_list  = det_boxes.split(boxes_per_image, 0)\n",
        "        det_scores_list = det_scores.split(boxes_per_image, 0)\n",
        "        det_labels_list = det_labels.split(boxes_per_image, 0)\n",
        "\n",
        "        det_all_boxes = []\n",
        "        det_all_scores = []\n",
        "        det_all_labels = []\n",
        "        \n",
        "\n",
        "        for boxes, scores, labels, ROI_image, tck_boxes_b, image_shape, original_im_shape in zip(det_boxes_list, \n",
        "                                                                         det_scores_list, \n",
        "                                                                         det_labels_list,\n",
        "                                                                         ROI_images,\n",
        "                                                                         tck_boxes,\n",
        "                                                                         images.image_sizes, \n",
        "                                                                         original_image_sizes):\n",
        "\n",
        "            boxes = clip_boxes_to_image(boxes, image_shape)\n",
        "\n",
        "            # batch everything, by making every class prediction be a separate instance\n",
        "            boxes = boxes.reshape(-1, 4)\n",
        "            scores = scores.reshape(-1)\n",
        "            labels = labels.reshape(-1)\n",
        "            \n",
        "            # remove low scoring boxes \n",
        "            inds = torch.nonzero(scores > self.det_score_thresh).squeeze(1)\n",
        "            boxes, scores, labels = boxes[inds], scores[inds], labels[inds]\n",
        "\n",
        "            # remove small boxes\n",
        "            keep = self.remove_small_boxes_area(boxes, min_size=self.det_min_area)\n",
        "            boxes, scores, labels = boxes[keep], scores[keep], labels[keep]\n",
        "            \n",
        "            # remove too big boxes\n",
        "            #keep = self.remove_big_boxes_area(boxes, max_size=self.det_max_area)\n",
        "            #boxes, scores, labels = boxes[keep], scores[keep], labels[keep]\n",
        "            \n",
        "            # non-maximum suppression, independently done per class\n",
        "            keep = nms(boxes, scores, self.det_nms_thresh)\n",
        "            #boxes, scores, labels = boxes[keep], scores[keep], labels[keep]\n",
        "            \n",
        "            # keep only topk scoring predictions\n",
        "            keep = keep[:self.roi_heads.detections_per_img]\n",
        "            boxes, scores, labels = boxes[keep], scores[keep], labels[keep]\n",
        "            \n",
        "            boxes = resize_boxes(boxes, image_shape, original_im_shape)\n",
        "            \n",
        "            if boxes.nelement():\n",
        "                keep = self.remove_boxes_out_roi(boxes, ROI_image, min_in_porcentage=self.det_min_ROI_in)\n",
        "                boxes, scores, labels = boxes[keep], scores[keep], labels[keep]\n",
        "                \n",
        "            # filter detections in tracks  \n",
        "            for tck_box in tck_boxes_b:\n",
        "                temp_boxes  = torch.cat([tck_box.unsqueeze(0), boxes])\n",
        "                temp_scores = torch.cat([torch.tensor([2.0]).to(boxes.device), scores])\n",
        "                keep   = nms(temp_boxes, temp_scores, self.det_nms_thresh)\n",
        "                keep   = keep[torch.ge(keep, 1)] - 1\n",
        "                boxes, scores, labels = boxes[keep], scores[keep], labels[keep]\n",
        "                if keep.nelement() == 0:\n",
        "                    break\n",
        "\n",
        "            \n",
        "            det_all_boxes.append(boxes)\n",
        "            det_all_scores.append(scores)\n",
        "            det_all_labels.append(labels)\n",
        "\n",
        "        return det_all_boxes, det_all_scores, det_all_labels\n",
        "\n",
        "\n",
        "    def tracking_processing(self,boxes, boxes_ids, images , features, ROI_images, original_image_sizes):\n",
        "        device = list(self.parameters())[0].device\n",
        "        # resize all the given box to be aligned\n",
        "        tck_proposals = [resize_boxes(box, or_size, size) for box, or_size, size in zip(boxes, \n",
        "                                                                                        original_image_sizes, \n",
        "                                                                                        images.image_sizes) if box.nelement()]\n",
        "        \n",
        "        tck_all_boxes  = []\n",
        "        tck_all_scores = []\n",
        "        tck_all_labels = []\n",
        "        tck_all_ids = []\n",
        "        \n",
        "        boxes_per_image = [boxes_in_image.shape[0] for boxes_in_image in tck_proposals]\n",
        "        \n",
        "        if tck_proposals:\n",
        "            tck_box_features = self.roi_heads.box_roi_pool(features, tck_proposals, images.image_sizes)\n",
        "            tck_box_features = self.roi_heads.box_head(tck_box_features)\n",
        "            tck_class_logits, tck_box_regression = self.roi_heads.box_predictor(tck_box_features)\n",
        "            tck_boxes = self.roi_heads.box_coder.decode(tck_box_regression, tck_proposals)\n",
        "            tck_scores = F.softmax(tck_class_logits, -1)\n",
        "\n",
        "            tck_scores, tck_labels = tck_scores[:,self.selected_classes].max(1)\n",
        "            tck_boxes = tck_boxes[:,self.selected_classes]\n",
        "            tck_boxes = torch.cat([tck_boxes[idx][i].unsqueeze(0)for idx, i in enumerate(tck_labels)])\n",
        "\n",
        "            tck_boxes_list  = tck_boxes.split(boxes_per_image, 0)\n",
        "            tck_scores_list = tck_scores.split(boxes_per_image, 0)\n",
        "            tck_labels_list = tck_labels.split(boxes_per_image, 0)\n",
        "            \n",
        "            for boxes, scores, labels, box_ids, ROI_image, image_shape, original_im_shape in zip(tck_boxes_list, \n",
        "                                                                         tck_scores_list, \n",
        "                                                                         tck_labels_list, \n",
        "                                                                         boxes_ids,\n",
        "                                                                         ROI_images,\n",
        "                                                                         images.image_sizes, \n",
        "                                                                         original_image_sizes):\n",
        "            \n",
        "                boxes = clip_boxes_to_image(boxes, image_shape)\n",
        "\n",
        "                # batch everything, by making every class prediction be a separate instance\n",
        "                boxes = boxes.reshape(-1, 4)\n",
        "                scores = scores.reshape(-1)\n",
        "                labels = labels.reshape(-1)\n",
        "                box_ids = box_ids.reshape(-1)\n",
        "\n",
        "                # remove low scoring boxes \n",
        "                keep    = torch.nonzero(scores > self.tck_score_thresh).squeeze(1)\n",
        "                boxes   = boxes[keep]\n",
        "                scores  = scores[keep]\n",
        "                labels  = labels[keep]\n",
        "                box_ids = box_ids[keep]\n",
        "\n",
        "                # remove small boxes\n",
        "                keep    = self.remove_small_boxes_area(boxes, min_size=self.tck_min_area)\n",
        "                boxes   = boxes[keep]\n",
        "                scores  = scores[keep]\n",
        "                labels  = labels[keep]\n",
        "                box_ids = box_ids[keep]\n",
        "                \n",
        "                # non-maximum suppression, independently done per class\n",
        "                keep    = nms(boxes, scores, self.tck_nms_thresh)\n",
        "                boxes   = boxes[keep]\n",
        "                scores  = scores[keep]\n",
        "                labels  = labels[keep]\n",
        "                box_ids = box_ids[keep]\n",
        "                # keep only topk scoring predictions\n",
        "\n",
        "                boxes = resize_boxes(boxes, image_shape, original_im_shape)\n",
        "                \n",
        "                if boxes.nelement():\n",
        "                    \n",
        "                    keep = self.remove_boxes_out_roi(boxes, ROI_image, min_in_porcentage=self.tck_min_ROI_in)\n",
        "                    boxes   = boxes[keep]\n",
        "                    scores  = scores[keep]\n",
        "                    labels  = labels[keep]\n",
        "                    box_ids = box_ids[keep]\n",
        "\n",
        "                tck_all_boxes.append(boxes)\n",
        "                tck_all_scores.append(scores)\n",
        "                tck_all_labels.append(labels)\n",
        "                tck_all_ids.append(box_ids)\n",
        "        else:\n",
        "            tck_all_boxes.append(torch.empty(0, device=device))\n",
        "            tck_all_scores.append(torch.empty(0, device=device))\n",
        "            tck_all_labels.append(torch.empty(0, device=device))\n",
        "            tck_all_ids.append(torch.empty(0, device=device))\n",
        "        \n",
        "        return tck_all_boxes,tck_all_scores,tck_all_labels, tck_all_ids\n",
        "           \n",
        "    def forward(self, images, boxes, boxes_ids, ROI_images):\n",
        "        # inputs to device\n",
        "        device = list(self.parameters())[0].device\n",
        "        \n",
        "        # get the original image size\n",
        "        original_image_sizes = [img.shape[-2:] for img in images]\n",
        "        \n",
        "        targets = None\n",
        "        # transform the images with the standart \n",
        "        images, targets = self.transform(images,targets)\n",
        "        # get the features form the transformd images\n",
        "        features = self.backbone(images.tensors)\n",
        "        if isinstance(features, torch.Tensor):\n",
        "            features = OrderedDict([(0, features)])\n",
        "            \n",
        "        # get the list of tracks\n",
        "        tck_boxes, tck_scores, tck_labels, tck_ids = self.tracking_processing(boxes, \n",
        "                                                                              boxes_ids, \n",
        "                                                                              images, \n",
        "                                                                              features, \n",
        "                                                                              ROI_images, \n",
        "                                                                              original_image_sizes)\n",
        "            \n",
        "        # get the list of detections\n",
        "        det_boxes, det_scores, det_labels = self.detections_processing(images, \n",
        "                                                                       features, \n",
        "                                                                       ROI_images, \n",
        "                                                                       tck_boxes, \n",
        "                                                                       original_image_sizes)\n",
        "        \n",
        "        return det_boxes, det_scores, det_labels, tck_boxes, tck_scores, tck_labels, tck_ids\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iVyluVh1FYU",
        "colab_type": "text"
      },
      "source": [
        "#Counting Classification: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJcP_OCB4Yf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def ccw2(A,B,C):\n",
        "    return (C[:,1]-A[:,1])*(B[:,0]-A[:,0]) > (B[:,1]-A[:,1])*(C[:,0]-A[:,0])\n",
        "\n",
        "def intersect2(A,B):\n",
        "    o1 = ccw2(A[:,0:2],B[:,0:2],B[:,2:4]) \n",
        "    o2 = ccw2(A[:,2:4],B[:,0:2],B[:,2:4]) \n",
        "    o3 = ccw2(A[:,0:2],A[:,2:4],B[:,0:2]) \n",
        "    o4 = ccw2(A[:,0:2],A[:,2:4],B[:,2:4])\n",
        "    return np.logical_and((o1 != o2),(o3 != o4)), o1\n",
        "\n",
        "class counter_classification:\n",
        "    def __init__(self, df_mov):\n",
        "\n",
        "        # get info numpy to optimize running time\n",
        "        self.calib_lines = df_mov.values[:,1:5].astype(int)\n",
        "        self.bool_guide  = df_mov.values[:,5].astype(bool)       \n",
        "        self.ids_movements = np.unique(df_mov.values[:,0]).astype(int)\n",
        "        self.direction_ref = df_mov.values[:,6].astype(bool)\n",
        "        self.bool_nan_dir = df_mov.values[:,6]!=df_mov.values[:,6]\n",
        "        # create a checklist to see if the conditions were met\n",
        "        self.check_list = np.zeros((self.ids_movements.shape[0],self.bool_guide .shape[0]), dtype=bool)\n",
        "        for i,id in enumerate(self.ids_movements):\n",
        "            self.check_list[i] = df_mov.values[:,0]==id\n",
        "\n",
        "    def classify(self,vector):\n",
        "\n",
        "        out = np.zeros((self.ids_movements.shape[0]),dtype=bool)\n",
        "        # create matrix with the input vector\n",
        "        vector2intersect = np.zeros_like(self.calib_lines,dtype=int)\n",
        "        vector2intersect[:] = vector\n",
        "        # intersect the vector with every calib line\n",
        "        res_inter, direction = intersect2(self.calib_lines,vector2intersect)\n",
        "        # check if the lines should intersec or not\n",
        "        bool_res = np.equal(res_inter, self.bool_guide)\n",
        "        # check if the detected cross is in the good direction\n",
        "        bool_direction = np.equal(self.direction_ref, direction)\n",
        "        # if we dont care put the dirction to true it to true\n",
        "        bool_direction[self.bool_nan_dir]=True\n",
        "        # check all the crossing conditions\n",
        "        for i,check in enumerate(self.check_list):\n",
        "            out[i]= bool_res[check].all() and bool_direction[check].all()\n",
        "        # create the output\n",
        "        if self.ids_movements[out].size == 0:\n",
        "            output = np.array([-1])\n",
        "        elif self.ids_movements[out].size > 1:\n",
        "            output = np.array([0])\n",
        "        else:\n",
        "            output = self.ids_movements[out]\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiRn5hzI1Iw2",
        "colab_type": "text"
      },
      "source": [
        "#Top 3: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAovRlBM43Jg",
        "colab_type": "text"
      },
      "source": [
        "#Detection: Mask RCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQMdPzo0Iy-R",
        "colab_type": "text"
      },
      "source": [
        "Google Colab Tutorials:\n",
        "\n",
        "[link text](https://github.com/matterport/Mask_RCNN/blob/master/samples/demo.ipynb)\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/matterport/Mask_RCNN/master/assets/street.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_gVyvTlJD82",
        "colab_type": "text"
      },
      "source": [
        "#Tracking: Toward-Realtime-MOT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk6XwSLjKhHr",
        "colab_type": "text"
      },
      "source": [
        "[link text](https://github.com/Lijun-Yu/zero_virus/tree/master/tracker)"
      ]
    }
  ]
}